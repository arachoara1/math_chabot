{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저와 모델 로드 시작\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\models--kresnik--wav2vec2-large-xlsr-korean. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'Wav2Vec2CTCTokenizer'. \n",
      "The class this function is called from is 'Wav2Vec2Tokenizer'.\n",
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\models\\wav2vec2\\tokenization_wav2vec2.py:720: FutureWarning: The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at kresnik/wav2vec2-large-xlsr-korean and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토크나이저와 모델 로드 완료: 13.646920442581177 초 소요\n",
      "오디오 로드 시작\n",
      "오디오 로드 완료: 7.066583156585693 초 소요\n",
      "오디오 데이터 변환 시작\n",
      "오디오 데이터 변환 완료: 2.6021485328674316 초 소요\n",
      "모델에 입력하여 변환 시작\n",
      "텍스트 변환 완료: 19.682043313980103 초 소요\n",
      "예측 값 추출 시작\n",
      "예측 값 추출 완료: 0.015625 초 소요\n",
      "문 자녀 나 뜻자 식도 생절하게 알려지는 생절한 양향 새  우리 지금지 각 에 리 리끼에 따라서 분 았 이번  간에는 이 가지 기준는 두 사용에서 산각경을 리해 볼거예요 온늘인 진나번 배는 내용이 많이 나오니까 다 자신을 꼭 재미께 부부 볼까우리 생각경 누가지 기준에 따라서 분를를 했고 천번째 기준는 사각을리 에 기리에 따라서 분리려는 것이었습니결에 길리 따라손는  려할 수 있어나 리 기서  한각기 세 는  보고 여기서 리가 생호 여빼마 있는 지를 살 왔리고 거기서 세면의 리 모두 달른  있고 동변의 끼가 같면 일  살같 세명의 끼가 모두같면 각경이라고 돌렀그리고 세 에 끼가 같면 당연히 두변의 계도 같니까  면 산각경라고도 한나고 했그래서 선 생임 여기에 있는 이 산각를 보고 이 등면 사감경과 세명의 끼리가 모두 른 산각으로 분리를 해 볼거예요먼서 단호떤 단는 어가요 른 떠니 여기에 있는 이 인병의 계리까 습니그렇다 이 생면 산각경이라고  수 있겠 나 어나는 세 에 끼리가 모두 달라따라서 두번째인 간에 써 주면 되겠네음단는 어떤가  여기   주변의 계가 같은으니까 이 등면 산각경라고 할 수 있고 는 세명 끼리가 모두 달라그래서 렇게 어고또  어가 많도 니요\n"
     ]
    }
   ],
   "source": [
    "import ffmpeg as ff\n",
    "import os\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer\n",
    "import torch\n",
    "import librosa\n",
    "import time  # 시간을 측정하기 위해 추가\n",
    "import tqdm\n",
    "\n",
    "\n",
    "# Wav2Vec 2.0 모델로 오디오 처리\n",
    "def transcribe_audio(audio_path):\n",
    "    # Wav2Vec 2.0 모델과 토크나이저 로드 (한국어 모델 사용)\n",
    "    print(\"토크나이저와 모델 로드 시작\")\n",
    "    start_time = time.time()\n",
    "    tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "    print(f\"토크나이저와 모델 로드 완료: {time.time() - start_time} 초 소요\")\n",
    "    \n",
    "    # 오디오 파일 로드 (librosa 사용)\n",
    "    print(\"오디오 로드 시작\")\n",
    "    start_time = time.time()\n",
    "    audio, rate = librosa.load(audio_path, sr=16000)  # 16000Hz로 샘플링\n",
    "    print(f\"오디오 로드 완료: {time.time() - start_time} 초 소요\")\n",
    "    \n",
    "    # 오디오 데이터를 모델 입력 형태로 변환\n",
    "    print(\"오디오 데이터 변환 시작\")\n",
    "    start_time = time.time()\n",
    "    input_values = tokenizer(audio, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "    print(f\"오디오 데이터 변환 완료: {time.time() - start_time} 초 소요\")\n",
    "    \n",
    "    # 모델에 입력하여 텍스트로 변환\n",
    "    print(\"모델에 입력하여 변환 시작\")\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "    print(f\"텍스트 변환 완료: {time.time() - start_time} 초 소요\")\n",
    "    \n",
    "    # 가장 높은 확률의 예측 값 추출\n",
    "    print(\"예측 값 추출 시작\")\n",
    "    start_time = time.time()\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    print(f\"예측 값 추출 완료: {time.time() - start_time} 초 소요\")\n",
    "    \n",
    "    # 텍스트로 변환\n",
    "    transcription = tokenizer.batch_decode(predicted_ids)[0]\n",
    "    return transcription\n",
    "\n",
    "# 예시 사용\n",
    "audio_path = './audio_output1.wav'  \n",
    "\n",
    "# 추출된 오디오를 Wav2Vec 2.0으로 처리\n",
    "transcript = transcribe_audio(audio_path)\n",
    "print(transcript)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
